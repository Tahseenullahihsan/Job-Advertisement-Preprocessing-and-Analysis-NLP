# -*- coding: utf-8 -*-
"""complete project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V0jeV7gwjANcPXcKFkiiEZKt1vn51T_J
"""

!pip install -U pip setuptools wheel
!pip install -U spacy
!python -m spacy download en_core_web_sm

from google.colab import files
import zipfile
import os

# Upload the zip file
uploaded = files.upload()

# Get the uploaded file name
zip_file_name = next(iter(uploaded))

# Create an extraction directory
extraction_dir = '/content/data'
os.makedirs(extraction_dir, exist_ok=True)

# Extract the zip file
with zipfile.ZipFile(zip_file_name, 'r') as zip_ref:
    zip_ref.extractall(extraction_dir)

# List the contents of the extraction directory
extracted_files = []
for root, dirs, files in os.walk(extraction_dir):
    for file in files:
        extracted_files.append(os.path.join(root, file))

extracted_files

import os
import spacy
import pandas as pd

# Load spaCy English model
nlp = spacy.load("en_core_web_sm")

# Define directory containing job advertisement files
data_dir = '/content/data'  # Make sure to upload your files to this path in Google Colab

# Ensure data directory exists
assert os.path.exists(data_dir), f"{data_dir} does not exist. Please upload your files to this path."

print("Environment setup complete.")

import os
import re
import pandas as pd

# Function to read job ads and extract relevant information
def extract_job_ads(data_dir):
    job_ads = []
    for root, _, files in os.walk(data_dir):
        for filename in files:
            file_path = os.path.join(root, filename)
            try:
                with open(file_path, 'r', encoding='utf-8') as file:
                    content = file.read()
            except UnicodeDecodeError:
                with open(file_path, 'r', encoding='latin1') as file:
                    content = file.read()

            # Example extraction logic
            title = re.search(r'Title: (.+)', content)
            webindex = re.search(r'Webindex: (.+)', content)
            description = re.search(r'Description: (.+)', content, re.DOTALL)

            if title and webindex and description:
                job_ads.append({
                    'title': title.group(1),
                    'webindex': webindex.group(1),
                    'description': description.group(1)
                })
    return pd.DataFrame(job_ads)

# Extract job advertisements
job_ads_df = extract_job_ads(data_dir)

# Display the extracted job advertisements
print(job_ads_df.head())

## for tokanization using tyhe regex expression and  tokeen into lower case
# Function to preprocess text using the specified regular expression
def preprocess_text(description):
    # Tokenize using the specified regular expression
    tokens = re.findall(r"[a-zA-Z]+(?:[-'][a-zA-Z]+)?", description)
    # Convert all words to lowercase
    tokens = [token.lower() for token in tokens]
    # Remove words with length less than 2
    tokens = [token for token in tokens if len(token) > 1]
    return tokens

# Apply preprocessing to each job description
job_ads_df['tokens'] = job_ads_df['description'].apply(preprocess_text)

# Display the tokenized job advertisements
print(job_ads_df[['title', 'tokens']].head())

# Path to the stopwords file
stopwords_path = '/content/data/Assignment5 data/stopwords_en.txt'

# Load stopwords from the file
with open(stopwords_path, 'r', encoding='utf-8') as file:
    stopwords = set(file.read().splitlines())

print(f"Loaded {len(stopwords)} stopwords.")

# Function to remove stopwords from tokens
def remove_stopwords(tokens):
    return [token for token in tokens if token not in stopwords]

# Apply stopword removal to each tokenized description
job_ads_df['filtered_tokens'] = job_ads_df['tokens'].apply(remove_stopwords)

# Display the job advertisements with stopwords removed
print(job_ads_df[['title', 'filtered_tokens']].head())

from collections import Counter

# Calculate term frequency of each word
all_words = [word for tokens in job_ads_df['filtered_tokens'] for word in tokens]
word_freq = Counter(all_words)

# Count words with frequency of one character
one_char_freq_count = sum(1 for word, freq in word_freq.items() if len(word) == 1 and freq == 1)

# Remove words that appear only once
filtered_tokens_freq = [[token for token in tokens if word_freq[token] > 1] for tokens in job_ads_df['filtered_tokens']]

# Update DataFrame with filtered tokens
job_ads_df['filtered_tokens_freq'] = filtered_tokens_freq

# Display the job advertisements with low-frequency words removed
print(job_ads_df[['title', 'filtered_tokens_freq']].head())

# Print count of words with frequency of one character
print(f"Count of words with frequency of one character: {one_char_freq_count}")

from collections import defaultdict

# Calculate document frequency of each word
document_freq = defaultdict(int)
for tokens in job_ads_df['filtered_tokens_freq']:
    for word in set(tokens):
        document_freq[word] += 1

# Remove the top 50 most frequent words based on document frequency
top_50_freq_words = sorted(document_freq.items(), key=lambda x: x[1], reverse=True)[:50]
top_50_freq_words = [word for word, _ in top_50_freq_words]

# Remove top 50 most frequent words from filtered tokens
filtered_tokens_freq_no_top_50 = [[token for token in tokens if token not in top_50_freq_words] for tokens in job_ads_df['filtered_tokens_freq']]

# Update DataFrame with filtered tokens
job_ads_df['filtered_tokens_no_top_50'] = filtered_tokens_freq_no_top_50

# Display the job advertisements with top 50 most frequent words removed
print(job_ads_df[['title', 'filtered_tokens_no_top_50']].head())

from google.colab import drive
drive.mount('/content/drive')

# Display the job advertisements with top 50 most frequent words removed
remaining_docs = job_ads_df[['title', 'filtered_tokens_no_top_50']]
print(remaining_docs.head())

# If you want to print all remaining documents:
print(remaining_docs)

# If you want to save the remaining documents to a file:
remaining_docs.to_csv('remaining_documents.csv', index=False)

# Save the pre-processed job advertisements text and information to a CSV file
output_file = '/content/data/preprocessed_job_ads.csv'
job_ads_df.to_csv(output_file, index=False, columns=['title', 'webindex', 'filtered_tokens'])

print(f"Pre-processed job advertisements saved to {output_file}.")

## building a vocabulary form the preproceesed data of jobs

# Load pre-processed job advertisements from CSV file
preprocessed_file = '/content/data/preprocessed_job_ads.csv'
job_ads_df = pd.read_csv(preprocessed_file)

# Extract all filtered tokens from the DataFrame
all_tokens = []
for tokens in job_ads_df['filtered_tokens']:
    # Convert the string representation of list back to list
    token_list = eval(tokens)
    all_tokens.extend(token_list)

# Create a set of unique words (vocabulary) from all tokens
vocab_set = set(all_tokens)

# Sort the vocabulary in alphabetical order
vocab_list = sorted(vocab_set)

# Create the vocabulary file with index starting from 0
vocab_file = '/content/data/vocab.txt'
with open(vocab_file, 'w', encoding='utf-8') as file:
    for index, word in enumerate(vocab_list):
        file.write(f"{index} {word}\n")

print(f"Vocabulary saved to {vocab_file}.")

### task 1 has beeen completed

### text pre proccesing and tokanizatiion , stop wordcs remover , high and low frequency worrds remove and building a vocabulary

""" Load Pre-processed Data and Vocabulary"""

import pandas as pd

# Load pre-processed job advertisements from CSV file
preprocessed_file = '/content/data/preprocessed_job_ads.csv'
job_ads_df = pd.read_csv(preprocessed_file)

# Load the vocabulary
vocab_file = '/content/data/vocab.txt'
vocab = {}
with open(vocab_file, 'r', encoding='utf-8') as file:
    for line in file:
        index, word = line.strip().split()
        vocab[word] = int(index)

print(f"Loaded vocabulary with {len(vocab)} words.")

### Create Bag-of-Words Representation

"""Create Bag-of-Words Representation"""

import numpy as np

def create_bow_vector(tokens, vocab):
    # Initialize a vector of zeros with the length of the vocabulary
    vector = np.zeros(len(vocab), dtype=int)
    # Count the occurrences of each token in the vocabulary
    for token in tokens:
        if token in vocab:
            vector[vocab[token]] += 1
    return vector

# Apply the Bag-of-Words model to each job description
job_ads_df['bow_vector'] = job_ads_df['filtered_tokens'].apply(lambda tokens: create_bow_vector(eval(tokens), vocab))

# Display the job advertisements with Bag-of-Words vectors
job_ads_df.head()

"""Save Count Vectors
Save the count vectors in count_vectors.txt in the specified format.
"""

# Save the Count vectors to a text file
count_vectors_file = '/content/data/count_vectors.txt'
with open(count_vectors_file, 'w', encoding='utf-8') as file:
    for index, row in job_ads_df.iterrows():
        vector_str = ' '.join(map(str, row['bow_vector']))
        file.write(f"{row['title']}\t{vector_str}\n")

print(f"Count vectors saved to {count_vectors_file}.")

"""1. Load Pre-trained Word Embeddings
We'll use GloVe pre-trained embeddings for this example. You can download the GloVe embeddings and upload them to your Colab environment.
"""

import numpy as np
import os

# Download GloVe embeddings (if not already present)
!wget http://nlp.stanford.edu/data/glove.6B.zip
!unzip -q glove.6B.zip -d glove.6B

# Load GloVe embeddings
def load_glove_embeddings(glove_file):
    embeddings_index = {}
    with open(glove_file, 'r', encoding='utf-8') as f:
        for line in f:
            values = line.split()
            word = values[0]
            vector = np.asarray(values[1:], dtype='float32')
            embeddings_index[word] = vector
    return embeddings_index

glove_file = 'glove.6B/glove.6B.300d.txt'  # Using the 300d version
embeddings_index = load_glove_embeddings(glove_file)

print(f"Loaded {len(embeddings_index)} word vectors.")

"""Generate Unweighted Vector Representations"""

# Function to compute unweighted vector representation
def compute_unweighted_vector(tokens, embeddings_index, vector_dim=300):
    vector = np.zeros(vector_dim)
    valid_tokens = 0
    for token in tokens:
        if token in embeddings_index:
            vector += embeddings_index[token]
            valid_tokens += 1
    if valid_tokens > 0:
        vector = vector / valid_tokens
    return vector

# Apply the function to each job description
job_ads_df['unweighted_vector'] = job_ads_df['filtered_tokens'].apply(lambda tokens: compute_unweighted_vector(eval(tokens), embeddings_index))

# Display the job advertisements with unweighted vectors
job_ads_df.head()

"""3. Generate TF-IDF Weighted Vector Representations"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Create a TF-IDF vectorizer
tfidf_vectorizer = TfidfVectorizer(tokenizer=lambda x: eval(x), preprocessor=lambda x: x)

# Fit the vectorizer on the tokenized descriptions
tfidf_matrix = tfidf_vectorizer.fit_transform(job_ads_df['filtered_tokens'])

# Create a mapping from words to their TF-IDF scores
feature_names = tfidf_vectorizer.get_feature_names_out()
tfidf_scores = {word: tfidf_matrix[:, idx].toarray().ravel() for idx, word in enumerate(feature_names)}

# Function to compute TF-IDF weighted vector representation
def compute_tfidf_weighted_vector(tokens, embeddings_index, tfidf_scores, vector_dim=300):
    vector = np.zeros(vector_dim)
    total_weight = 0
    for token in tokens:
        if token in embeddings_index and token in tfidf_scores:
            weight = tfidf_scores[token].mean()  # Mean TF-IDF score for the token
            vector += weight * embeddings_index[token]
            total_weight += weight
    if total_weight > 0:
        vector = vector / total_weight
    return vector

# Apply the function to each job description
job_ads_df['tfidf_weighted_vector'] = job_ads_df['filtered_tokens'].apply(lambda tokens: compute_tfidf_weighted_vector(eval(tokens), embeddings_index, tfidf_scores))

# Display the job advertisements with TF-IDF weighted vectors
job_ads_df.head()

"""Save the Representations for Future Use"""

# Save the unweighted and TF-IDF weighted vectors to CSV files
unweighted_vectors_file = '/content/data/unweighted_vectors.csv'
tfidf_weighted_vectors_file = '/content/data/tfidf_weighted_vectors.csv'

job_ads_df[['title', 'unweighted_vector']].to_csv(unweighted_vectors_file, index=False)
job_ads_df[['title', 'tfidf_weighted_vector']].to_csv(tfidf_weighted_vectors_file, index=False)

print(f"Unweighted vectors saved to {unweighted_vectors_file}.")
print(f"TF-IDF weighted vectors saved to {tfidf_weighted_vectors_file}.")

"""task 2 hase been completed"""

#1. Load the Feature Representations
#Load the unweighted and TF-IDF weighted vectors from the CSV files generated in Task 2.

import pandas as pd

# Function to convert string representation of vectors to list of floats
def parse_vector(vector_str):
    # Remove square brackets and split the string by spaces
    vector_str = vector_str.strip('[]')
    return [float(num) for num in vector_str.split()]

# Load the unweighted vectors
unweighted_vectors_file = '/content/data/unweighted_vectors.csv'
unweighted_df = pd.read_csv(unweighted_vectors_file)

# Load the TF-IDF weighted vectors
tfidf_weighted_vectors_file = '/content/data/tfidf_weighted_vectors.csv'
tfidf_weighted_df = pd.read_csv(tfidf_weighted_vectors_file)

# Assume that 'title' column is the label (you can change this based on your actual data)
# Extract labels
labels = unweighted_df['title']  # Replace 'title' with the actual label column if different

# Extract features
unweighted_vectors = unweighted_df['unweighted_vector'].apply(parse_vector).tolist()
tfidf_weighted_vectors = tfidf_weighted_df['tfidf_weighted_vector'].apply(parse_vector).tolist()

print(f"Loaded {len(unweighted_vectors)} unweighted vectors and {len(tfidf_weighted_vectors)} TF-IDF weighted vectors.")

"""2. Train Logistic Regression Models
We will use the LogisticRegression model from scikit-learn.
"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score, KFold
from sklearn.metrics import accuracy_score, precision_score, recall_score, make_scorer

# Initialize the logistic regression model
logreg = LogisticRegression(max_iter=1000, random_state=42)

# Define custom scoring functions for precision and recall
scoring = {
    'accuracy': make_scorer(accuracy_score),
    'precision': make_scorer(precision_score, average='macro', zero_division=0),
    'recall': make_scorer(recall_score, average='macro', zero_division=0)
}

# Function to perform 5-fold cross-validation and return mean scores
def evaluate_model(model, X, y):
    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    accuracy_scores = []
    precision_scores = []
    recall_scores = []

    for train_index, test_index in kf.split(X):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        accuracy_scores.append(accuracy_score(y_test, y_pred))
        precision_scores.append(precision_score(y_test, y_pred, average='macro', zero_division=0))
        recall_scores.append(recall_score(y_test, y_pred, average='macro', zero_division=0))

        # Debug prints
        print(f"Fold results - Accuracy: {accuracy_scores[-1]:.4f}, Precision: {precision_scores[-1]:.4f}, Recall: {recall_scores[-1]:.4f}")

    return np.mean(accuracy_scores), np.mean(precision_scores), np.mean(recall_scores)

# Prepare the feature arrays and labels
X_unweighted = np.array(unweighted_vectors)
X_tfidf_weighted = np.array(tfidf_weighted_vectors)
y = labels

# Evaluate the model with unweighted vectors
acc_unweighted, prec_unweighted, rec_unweighted = evaluate_model(logreg, X_unweighted, y)
print(f"Unweighted Vectors - Accuracy: {acc_unweighted:.4f}, Precision: {prec_unweighted:.4f}, Recall: {rec_unweighted:.4f}")

# Evaluate the model with TF-IDF weighted vectors
acc_tfidf_weighted, prec_tfidf_weighted, rec_tfidf_weighted = evaluate_model(logreg, X_tfidf_weighted, y)
print(f"TF-IDF Weighted Vectors - Accuracy: {acc_tfidf_weighted:.4f}, Precision: {prec_tfidf_weighted:.4f}, Recall: {rec_tfidf_weighted:.4f}")

"""Compare the Performance Metrics
Print and compare the performance metrics for both unweighted and TF-IDF weighted vectors.
"""

# Print the comparison results
print("Comparison of Logistic Regression Model Performance:")
print("Unweighted Vectors:")
print(f"  Accuracy:  {acc_unweighted:.4f}")
print(f"  Precision: {prec_unweighted:.4f}")
print(f"  Recall:    {rec_unweighted:.4f}")

print("TF-IDF Weighted Vectors:")
print(f"  Accuracy:  {acc_tfidf_weighted:.4f}")
print(f"  Precision: {prec_tfidf_weighted:.4f}")
print(f"  Recall:    {rec_tfidf_weighted:.4f}")

#### Debugging the model

# Verify Labels
print("Label distribution:")
print(labels.value_counts())

print("Sample unweighted vectors:")
print(unweighted_vectors[:5])

print("Sample TF-IDF weighted vectors:")
print(tfidf_weighted_vectors[:5])

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score, KFold
from sklearn.metrics import accuracy_score, precision_score, recall_score, make_scorer

# Initialize the logistic regression model
logreg = LogisticRegression(max_iter=1000, random_state=42)

# Define custom scoring functions for precision and recall
scoring = {
    'accuracy': make_scorer(accuracy_score),
    'precision': make_scorer(precision_score, average='macro', zero_division=0),
    'recall': make_scorer(recall_score, average='macro', zero_division=0)
}

# Function to perform 5-fold cross-validation and return mean scores
def evaluate_model(model, X, y):
    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    accuracy_scores = []
    precision_scores = []
    recall_scores = []

    for train_index, test_index in kf.split(X):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        accuracy_scores.append(accuracy_score(y_test, y_pred))
        precision_scores.append(precision_score(y_test, y_pred, average='macro', zero_division=0))
        recall_scores.append(recall_score(y_test, y_pred, average='macro', zero_division=0))

        # Debug prints
        print(f"Fold results - Accuracy: {accuracy_scores[-1]:.4f}, Precision: {precision_scores[-1]:.4f}, Recall: {recall_scores[-1]:.4f}")

    return np.mean(accuracy_scores), np.mean(precision_scores), np.mean(recall_scores)

# Prepare the feature arrays and labels
X_unweighted = np.array(unweighted_vectors)
X_tfidf_weighted = np.array(tfidf_weighted_vectors)
y = labels

# Evaluate the model with unweighted vectors
acc_unweighted, prec_unweighted, rec_unweighted = evaluate_model(logreg, X_unweighted, y)
print(f"Unweighted Vectors - Accuracy: {acc_unweighted:.4f}, Precision: {prec_unweighted:.4f}, Recall: {rec_unweighted:.4f}")

# Evaluate the model with TF-IDF weighted vectors
acc_tfidf_weighted, prec_tfidf_weighted, rec_tfidf_weighted = evaluate_model(logreg, X_tfidf_weighted, y)
print(f"TF-IDF Weighted Vectors - Accuracy: {acc_tfidf_weighted:.4f}, Precision: {prec_tfidf_weighted:.4f}, Recall: {rec_tfidf_weighted:.4f}")

print("Label distribution:")
print(y.value_counts())

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, KFold
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Load the preprocessed job ads data
file_path = '/content/data/preprocessed_job_ads.csv'
data = pd.read_csv(file_path)

# Convert filtered tokens back to a single string format for each job ad
data['description'] = data['filtered_tokens'].apply(lambda x: ' '.join(eval(x)))

# Since we don't have actual categories, let's simulate some for demonstration purposes
# In a real scenario, this should be replaced with actual job categories
categories = ['Accounting_Finance', 'Engineering', 'Healthcare_Nursing', 'Sales']
data['category'] = np.random.choice(categories, size=len(data))

# Extract features and labels
X = data['description']
y = data['category']

# Verify the distribution of the labels
print("Label distribution:")
print(y.value_counts())

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Vectorize the text data using TF-IDF
tfidf_vectorizer = TfidfVectorizer(max_features=1000)
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

# Train a Random Forest classifier
random_forest = RandomForestClassifier(n_estimators=100, random_state=42)
random_forest.fit(X_train_tfidf, y_train)

# Predict the labels for the test set
y_pred = random_forest.predict(X_test_tfidf)

# Evaluate the model
print("Classification Report:")
print(classification_report(y_test, y_pred))

print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

print("Accuracy Score:")
print(accuracy_score(y_test, y_pred))

